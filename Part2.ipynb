{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is a neural network implementation that learns distributed representations for words / distributed word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__import data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unlabeled_train = pd.read_csv(\"data/unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"review\"].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"review\"].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_train.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__clean data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    # Remove HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    # Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    # Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    # Remove stopwords\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Word2Vec expects sentences as lists of words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load punkt tokenizer to break up a paragraph into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    '''\n",
    "    Function to split a review into parsed sentences. Returns a list of sentences, \n",
    "    where each sentence is a list of words. \n",
    "    '''\n",
    "    # Creates list of strings (sentences)\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    # convert list of strings into list of list of words\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if(len(raw_sentence) > 0):\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Christian/anaconda/envs/dlmovies/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file /Users/Christian/anaconda/envs/dlmovies/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"html.parser\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "/Users/Christian/anaconda/envs/dlmovies/lib/python3.5/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "/Users/Christian/anaconda/envs/dlmovies/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Christian/anaconda/envs/dlmovies/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file /Users/Christian/anaconda/envs/dlmovies/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"html.parser\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "/Users/Christian/anaconda/envs/dlmovies/lib/python3.5/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "/Users/Christian/anaconda/envs/dlmovies/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/Christian/anaconda/envs/dlmovies/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/Christian/anaconda/envs/dlmovies/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/Christian/anaconda/envs/dlmovies/lib/python3.5/site-packages/bs4/__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "/Users/Christian/anaconda/envs/dlmovies/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/Users/Christian/anaconda/envs/dlmovies/lib/python3.5/site-packages/bs4/__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1062089"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all reviews have been converted to sentences; one sentences is one list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again'], ['maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent'], ['moonwalker', 'is', 'part', 'biography', 'part', 'feature', 'film', 'which', 'i', 'remember', 'going', 'to', 'see', 'at', 'the', 'cinema', 'when', 'it', 'was', 'originally', 'released'], ['some', 'of', 'it', 'has', 'subtle', 'messages', 'about', 'mj', 's', 'feeling', 'towards', 'the', 'press', 'and', 'also', 'the', 'obvious', 'message', 'of', 'drugs', 'are', 'bad', 'm', 'kay', 'visually', 'impressive', 'but', 'of', 'course', 'this', 'is', 'all', 'about', 'michael', 'jackson', 'so', 'unless', 'you', 'remotely', 'like', 'mj', 'in', 'anyway', 'then', 'you', 'are', 'going', 'to', 'hate', 'this', 'and', 'find', 'it', 'boring'], ['some', 'may', 'call', 'mj', 'an', 'egotist', 'for', 'consenting', 'to', 'the', 'making', 'of', 'this', 'movie', 'but', 'mj', 'and', 'most', 'of', 'his', 'fans', 'would', 'say', 'that', 'he', 'made', 'it', 'for', 'the', 'fans', 'which', 'if', 'true', 'is', 'really', 'nice', 'of', 'him', 'the', 'actual', 'feature', 'film', 'bit', 'when', 'it', 'finally', 'starts', 'is', 'only', 'on', 'for', 'minutes', 'or', 'so', 'excluding', 'the', 'smooth', 'criminal', 'sequence', 'and', 'joe', 'pesci', 'is', 'convincing', 'as', 'a', 'psychopathic', 'all', 'powerful', 'drug', 'lord'], ['why', 'he', 'wants', 'mj', 'dead', 'so', 'bad', 'is', 'beyond', 'me'], ['because', 'mj', 'overheard', 'his', 'plans'], ['nah', 'joe', 'pesci', 's', 'character', 'ranted', 'that', 'he', 'wanted', 'people', 'to', 'know', 'it', 'is', 'he', 'who', 'is', 'supplying', 'drugs', 'etc', 'so', 'i', 'dunno', 'maybe', 'he', 'just', 'hates', 'mj', 's', 'music', 'lots', 'of', 'cool', 'things', 'in', 'this', 'like', 'mj', 'turning', 'into', 'a', 'car', 'and', 'a', 'robot', 'and', 'the', 'whole', 'speed', 'demon', 'sequence'], ['also', 'the', 'director', 'must', 'have', 'had', 'the', 'patience', 'of', 'a', 'saint', 'when', 'it', 'came', 'to', 'filming', 'the', 'kiddy', 'bad', 'sequence', 'as', 'usually', 'directors', 'hate', 'working', 'with', 'one', 'kid', 'let', 'alone', 'a', 'whole', 'bunch', 'of', 'them', 'performing', 'a', 'complex', 'dance', 'scene', 'bottom', 'line', 'this', 'movie', 'is', 'for', 'people', 'who', 'like', 'mj', 'on', 'one', 'level', 'or', 'another', 'which', 'i', 'think', 'is', 'most', 'people'], ['if', 'not', 'then', 'stay', 'away']]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training and saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = 300 # word vector dim\n",
    "min_word_count = 40\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-08 14:48:36,677 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-08 14:48:36,687 : INFO : collecting all words and their counts\n",
      "2017-02-08 14:48:36,688 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-02-08 14:48:36,740 : INFO : PROGRESS: at sentence #10000, processed 225803 words, keeping 17776 word types\n",
      "2017-02-08 14:48:36,792 : INFO : PROGRESS: at sentence #20000, processed 451892 words, keeping 24948 word types\n",
      "2017-02-08 14:48:36,845 : INFO : PROGRESS: at sentence #30000, processed 671314 words, keeping 30034 word types\n",
      "2017-02-08 14:48:36,899 : INFO : PROGRESS: at sentence #40000, processed 897814 words, keeping 34348 word types\n",
      "2017-02-08 14:48:36,953 : INFO : PROGRESS: at sentence #50000, processed 1116962 words, keeping 37761 word types\n",
      "2017-02-08 14:48:37,012 : INFO : PROGRESS: at sentence #60000, processed 1338403 words, keeping 40723 word types\n",
      "2017-02-08 14:48:37,067 : INFO : PROGRESS: at sentence #70000, processed 1561579 words, keeping 43333 word types\n",
      "2017-02-08 14:48:37,123 : INFO : PROGRESS: at sentence #80000, processed 1780886 words, keeping 45714 word types\n",
      "2017-02-08 14:48:37,178 : INFO : PROGRESS: at sentence #90000, processed 2004995 words, keeping 48135 word types\n",
      "2017-02-08 14:48:37,232 : INFO : PROGRESS: at sentence #100000, processed 2226966 words, keeping 50207 word types\n",
      "2017-02-08 14:48:37,287 : INFO : PROGRESS: at sentence #110000, processed 2446580 words, keeping 52081 word types\n",
      "2017-02-08 14:48:37,345 : INFO : PROGRESS: at sentence #120000, processed 2668775 words, keeping 54119 word types\n",
      "2017-02-08 14:48:37,403 : INFO : PROGRESS: at sentence #130000, processed 2894303 words, keeping 55847 word types\n",
      "2017-02-08 14:48:37,456 : INFO : PROGRESS: at sentence #140000, processed 3107005 words, keeping 57346 word types\n",
      "2017-02-08 14:48:37,510 : INFO : PROGRESS: at sentence #150000, processed 3332627 words, keeping 59055 word types\n",
      "2017-02-08 14:48:37,566 : INFO : PROGRESS: at sentence #160000, processed 3555315 words, keeping 60617 word types\n",
      "2017-02-08 14:48:37,624 : INFO : PROGRESS: at sentence #170000, processed 3778655 words, keeping 62077 word types\n",
      "2017-02-08 14:48:37,676 : INFO : PROGRESS: at sentence #180000, processed 3999236 words, keeping 63496 word types\n",
      "2017-02-08 14:48:37,731 : INFO : PROGRESS: at sentence #190000, processed 4224449 words, keeping 64794 word types\n",
      "2017-02-08 14:48:37,784 : INFO : PROGRESS: at sentence #200000, processed 4448603 words, keeping 66087 word types\n",
      "2017-02-08 14:48:37,841 : INFO : PROGRESS: at sentence #210000, processed 4669967 words, keeping 67390 word types\n",
      "2017-02-08 14:48:37,900 : INFO : PROGRESS: at sentence #220000, processed 4894968 words, keeping 68697 word types\n",
      "2017-02-08 14:48:37,953 : INFO : PROGRESS: at sentence #230000, processed 5117545 words, keeping 69958 word types\n",
      "2017-02-08 14:48:38,007 : INFO : PROGRESS: at sentence #240000, processed 5345050 words, keeping 71167 word types\n",
      "2017-02-08 14:48:38,058 : INFO : PROGRESS: at sentence #250000, processed 5559165 words, keeping 72351 word types\n",
      "2017-02-08 14:48:38,108 : INFO : PROGRESS: at sentence #260000, processed 5779146 words, keeping 73478 word types\n",
      "2017-02-08 14:48:38,160 : INFO : PROGRESS: at sentence #270000, processed 5999168 words, keeping 74218 word types\n",
      "2017-02-08 14:48:38,219 : INFO : PROGRESS: at sentence #280000, processed 6223909 words, keeping 74218 word types\n",
      "2017-02-08 14:48:38,275 : INFO : PROGRESS: at sentence #290000, processed 6447077 words, keeping 74218 word types\n",
      "2017-02-08 14:48:38,329 : INFO : PROGRESS: at sentence #300000, processed 6668822 words, keeping 74218 word types\n",
      "2017-02-08 14:48:38,387 : INFO : PROGRESS: at sentence #310000, processed 6895779 words, keeping 74218 word types\n",
      "2017-02-08 14:48:38,442 : INFO : PROGRESS: at sentence #320000, processed 7114991 words, keeping 74218 word types\n",
      "2017-02-08 14:48:38,496 : INFO : PROGRESS: at sentence #330000, processed 7333809 words, keeping 74218 word types\n",
      "2017-02-08 14:48:38,557 : INFO : PROGRESS: at sentence #340000, processed 7557463 words, keeping 74218 word types\n",
      "2017-02-08 14:48:38,612 : INFO : PROGRESS: at sentence #350000, processed 7781085 words, keeping 74218 word types\n",
      "2017-02-08 14:48:38,668 : INFO : PROGRESS: at sentence #360000, processed 7999950 words, keeping 74218 word types\n",
      "2017-02-08 14:48:38,723 : INFO : PROGRESS: at sentence #370000, processed 8221068 words, keeping 74218 word types\n",
      "2017-02-08 14:48:38,783 : INFO : PROGRESS: at sentence #380000, processed 8442425 words, keeping 74218 word types\n",
      "2017-02-08 14:48:38,837 : INFO : PROGRESS: at sentence #390000, processed 8668136 words, keeping 74218 word types\n",
      "2017-02-08 14:48:38,889 : INFO : PROGRESS: at sentence #400000, processed 8884784 words, keeping 74218 word types\n",
      "2017-02-08 14:48:38,946 : INFO : PROGRESS: at sentence #410000, processed 9107348 words, keeping 74218 word types\n",
      "2017-02-08 14:48:39,005 : INFO : PROGRESS: at sentence #420000, processed 9328499 words, keeping 74218 word types\n",
      "2017-02-08 14:48:39,063 : INFO : PROGRESS: at sentence #430000, processed 9550879 words, keeping 74218 word types\n",
      "2017-02-08 14:48:39,120 : INFO : PROGRESS: at sentence #440000, processed 9773932 words, keeping 74218 word types\n",
      "2017-02-08 14:48:39,173 : INFO : PROGRESS: at sentence #450000, processed 9997826 words, keeping 74218 word types\n",
      "2017-02-08 14:48:39,229 : INFO : PROGRESS: at sentence #460000, processed 10221127 words, keeping 74218 word types\n",
      "2017-02-08 14:48:39,288 : INFO : PROGRESS: at sentence #470000, processed 10444168 words, keeping 74218 word types\n",
      "2017-02-08 14:48:39,348 : INFO : PROGRESS: at sentence #480000, processed 10667917 words, keeping 74218 word types\n",
      "2017-02-08 14:48:39,403 : INFO : PROGRESS: at sentence #490000, processed 10892775 words, keeping 74218 word types\n",
      "2017-02-08 14:48:39,456 : INFO : PROGRESS: at sentence #500000, processed 11118453 words, keeping 74218 word types\n",
      "2017-02-08 14:48:39,511 : INFO : PROGRESS: at sentence #510000, processed 11339041 words, keeping 74218 word types\n",
      "2017-02-08 14:48:39,564 : INFO : PROGRESS: at sentence #520000, processed 11553753 words, keeping 74218 word types\n",
      "2017-02-08 14:48:39,618 : INFO : PROGRESS: at sentence #530000, processed 11774180 words, keeping 74218 word types\n",
      "2017-02-08 14:48:39,676 : INFO : PROGRESS: at sentence #540000, processed 11997971 words, keeping 75302 word types\n",
      "2017-02-08 14:48:39,732 : INFO : PROGRESS: at sentence #550000, processed 12222695 words, keeping 76886 word types\n",
      "2017-02-08 14:48:39,786 : INFO : PROGRESS: at sentence #560000, processed 12447130 words, keeping 78282 word types\n",
      "2017-02-08 14:48:39,844 : INFO : PROGRESS: at sentence #570000, processed 12673368 words, keeping 79638 word types\n",
      "2017-02-08 14:48:39,898 : INFO : PROGRESS: at sentence #580000, processed 12894566 words, keeping 80925 word types\n",
      "2017-02-08 14:48:39,955 : INFO : PROGRESS: at sentence #590000, processed 13121988 words, keeping 82220 word types\n",
      "2017-02-08 14:48:40,015 : INFO : PROGRESS: at sentence #600000, processed 13346707 words, keeping 83441 word types\n",
      "2017-02-08 14:48:40,076 : INFO : PROGRESS: at sentence #610000, processed 13573182 words, keeping 84715 word types\n",
      "2017-02-08 14:48:40,136 : INFO : PROGRESS: at sentence #620000, processed 13794609 words, keeping 85840 word types\n",
      "2017-02-08 14:48:40,197 : INFO : PROGRESS: at sentence #630000, processed 14021036 words, keeping 86965 word types\n",
      "2017-02-08 14:48:40,258 : INFO : PROGRESS: at sentence #640000, processed 14247571 words, keeping 88135 word types\n",
      "2017-02-08 14:48:40,318 : INFO : PROGRESS: at sentence #650000, processed 14472552 words, keeping 89240 word types\n",
      "2017-02-08 14:48:40,378 : INFO : PROGRESS: at sentence #660000, processed 14698623 words, keeping 90257 word types\n",
      "2017-02-08 14:48:40,438 : INFO : PROGRESS: at sentence #670000, processed 14921004 words, keeping 91265 word types\n",
      "2017-02-08 14:48:40,498 : INFO : PROGRESS: at sentence #680000, processed 15142328 words, keeping 92243 word types\n",
      "2017-02-08 14:48:40,562 : INFO : PROGRESS: at sentence #690000, processed 15366513 words, keeping 93271 word types\n",
      "2017-02-08 14:48:40,620 : INFO : PROGRESS: at sentence #700000, processed 15593570 words, keeping 94278 word types\n",
      "2017-02-08 14:48:40,674 : INFO : PROGRESS: at sentence #710000, processed 15817816 words, keeping 95321 word types\n",
      "2017-02-08 14:48:40,735 : INFO : PROGRESS: at sentence #720000, processed 16045125 words, keeping 96356 word types\n",
      "2017-02-08 14:48:40,794 : INFO : PROGRESS: at sentence #730000, processed 16275749 words, keeping 97374 word types\n",
      "2017-02-08 14:48:40,855 : INFO : PROGRESS: at sentence #740000, processed 16503401 words, keeping 98265 word types\n",
      "2017-02-08 14:48:40,914 : INFO : PROGRESS: at sentence #750000, processed 16726006 words, keeping 99245 word types\n",
      "2017-02-08 14:48:40,971 : INFO : PROGRESS: at sentence #760000, processed 16951493 words, keeping 100165 word types\n",
      "2017-02-08 14:48:41,033 : INFO : PROGRESS: at sentence #770000, processed 17172638 words, keeping 101096 word types\n",
      "2017-02-08 14:48:41,095 : INFO : PROGRESS: at sentence #780000, processed 17397095 words, keeping 101983 word types\n",
      "2017-02-08 14:48:41,158 : INFO : PROGRESS: at sentence #790000, processed 17622151 words, keeping 102867 word types\n",
      "2017-02-08 14:48:41,217 : INFO : PROGRESS: at sentence #800000, processed 17848795 words, keeping 103719 word types\n",
      "2017-02-08 14:48:41,278 : INFO : PROGRESS: at sentence #810000, processed 18067635 words, keeping 104576 word types\n",
      "2017-02-08 14:48:41,335 : INFO : PROGRESS: at sentence #820000, processed 18294545 words, keeping 105427 word types\n",
      "2017-02-08 14:48:41,391 : INFO : PROGRESS: at sentence #830000, processed 18519010 words, keeping 106249 word types\n",
      "2017-02-08 14:48:41,449 : INFO : PROGRESS: at sentence #840000, processed 18746462 words, keeping 107121 word types\n",
      "2017-02-08 14:48:41,503 : INFO : PROGRESS: at sentence #850000, processed 18968071 words, keeping 107926 word types\n",
      "2017-02-08 14:48:41,560 : INFO : PROGRESS: at sentence #860000, processed 19191652 words, keeping 108721 word types\n",
      "2017-02-08 14:48:41,616 : INFO : PROGRESS: at sentence #870000, processed 19412370 words, keeping 109501 word types\n",
      "2017-02-08 14:48:41,671 : INFO : PROGRESS: at sentence #880000, processed 19638220 words, keeping 110354 word types\n",
      "2017-02-08 14:48:41,727 : INFO : PROGRESS: at sentence #890000, processed 19865766 words, keeping 111142 word types\n",
      "2017-02-08 14:48:41,782 : INFO : PROGRESS: at sentence #900000, processed 20084631 words, keeping 111905 word types\n",
      "2017-02-08 14:48:41,837 : INFO : PROGRESS: at sentence #910000, processed 20307195 words, keeping 112688 word types\n",
      "2017-02-08 14:48:41,896 : INFO : PROGRESS: at sentence #920000, processed 20532638 words, keeping 113450 word types\n",
      "2017-02-08 14:48:41,951 : INFO : PROGRESS: at sentence #930000, processed 20755246 words, keeping 114198 word types\n",
      "2017-02-08 14:48:42,008 : INFO : PROGRESS: at sentence #940000, processed 20981063 words, keeping 114901 word types\n",
      "2017-02-08 14:48:42,063 : INFO : PROGRESS: at sentence #950000, processed 21204315 words, keeping 115606 word types\n",
      "2017-02-08 14:48:42,120 : INFO : PROGRESS: at sentence #960000, processed 21427712 words, keeping 116407 word types\n",
      "2017-02-08 14:48:42,180 : INFO : PROGRESS: at sentence #970000, processed 21657140 words, keeping 117169 word types\n",
      "2017-02-08 14:48:42,235 : INFO : PROGRESS: at sentence #980000, processed 21882293 words, keeping 117832 word types\n",
      "2017-02-08 14:48:42,289 : INFO : PROGRESS: at sentence #990000, processed 22103949 words, keeping 118502 word types\n",
      "2017-02-08 14:48:42,345 : INFO : PROGRESS: at sentence #1000000, processed 22328951 words, keeping 119188 word types\n",
      "2017-02-08 14:48:42,399 : INFO : PROGRESS: at sentence #1010000, processed 22550587 words, keeping 119883 word types\n",
      "2017-02-08 14:48:42,456 : INFO : PROGRESS: at sentence #1020000, processed 22770610 words, keeping 120545 word types\n",
      "2017-02-08 14:48:42,518 : INFO : PROGRESS: at sentence #1030000, processed 22990002 words, keeping 121201 word types\n",
      "2017-02-08 14:48:42,576 : INFO : PROGRESS: at sentence #1040000, processed 23218421 words, keeping 121921 word types\n",
      "2017-02-08 14:48:42,638 : INFO : PROGRESS: at sentence #1050000, processed 23445509 words, keeping 122612 word types\n",
      "2017-02-08 14:48:42,695 : INFO : PROGRESS: at sentence #1060000, processed 23673486 words, keeping 123363 word types\n",
      "2017-02-08 14:48:42,712 : INFO : collected 123504 word types from a corpus of 23718987 raw words and 1062089 sentences\n",
      "2017-02-08 14:48:42,713 : INFO : Loading a fresh vocabulary\n",
      "2017-02-08 14:48:42,827 : INFO : min_count=40 retains 19367 unique words (15% of original 123504, drops 104137)\n",
      "2017-02-08 14:48:42,828 : INFO : min_count=40 leaves 23108996 word corpus (97% of original 23718987, drops 609991)\n",
      "2017-02-08 14:48:42,885 : INFO : deleting the raw counts dictionary of 123504 items\n",
      "2017-02-08 14:48:42,892 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2017-02-08 14:48:42,893 : INFO : downsampling leaves estimated 17140307 word corpus (74.2% of prior 23108996)\n",
      "2017-02-08 14:48:42,894 : INFO : estimated required memory for 19367 words and 300 dimensions: 56164300 bytes\n",
      "2017-02-08 14:48:42,957 : INFO : resetting layer weights\n",
      "2017-02-08 14:48:43,257 : INFO : training model with 4 workers on 19367 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-02-08 14:48:43,257 : INFO : expecting 1062089 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-02-08 14:48:44,266 : INFO : PROGRESS: at 1.26% examples, 1082596 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:48:45,273 : INFO : PROGRESS: at 2.57% examples, 1094677 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:48:46,278 : INFO : PROGRESS: at 3.90% examples, 1105856 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:48:47,285 : INFO : PROGRESS: at 5.20% examples, 1105379 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:48:48,296 : INFO : PROGRESS: at 6.54% examples, 1109911 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:48:49,298 : INFO : PROGRESS: at 7.85% examples, 1111182 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:48:50,300 : INFO : PROGRESS: at 9.16% examples, 1112937 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:48:51,303 : INFO : PROGRESS: at 10.47% examples, 1112226 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:48:52,303 : INFO : PROGRESS: at 11.78% examples, 1114416 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:48:53,306 : INFO : PROGRESS: at 13.04% examples, 1110657 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:48:54,308 : INFO : PROGRESS: at 14.06% examples, 1090067 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:48:55,316 : INFO : PROGRESS: at 15.20% examples, 1080243 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:48:56,320 : INFO : PROGRESS: at 16.45% examples, 1079497 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:48:57,322 : INFO : PROGRESS: at 17.81% examples, 1085595 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:48:58,326 : INFO : PROGRESS: at 19.12% examples, 1087770 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:48:59,330 : INFO : PROGRESS: at 20.38% examples, 1087568 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:00,332 : INFO : PROGRESS: at 21.49% examples, 1079135 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:01,336 : INFO : PROGRESS: at 22.44% examples, 1064032 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:02,353 : INFO : PROGRESS: at 23.52% examples, 1055752 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:03,355 : INFO : PROGRESS: at 24.56% examples, 1047277 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:04,356 : INFO : PROGRESS: at 25.70% examples, 1043786 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:05,357 : INFO : PROGRESS: at 26.99% examples, 1046188 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:06,360 : INFO : PROGRESS: at 27.99% examples, 1037637 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:07,376 : INFO : PROGRESS: at 28.88% examples, 1025370 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:08,382 : INFO : PROGRESS: at 29.90% examples, 1019064 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:09,390 : INFO : PROGRESS: at 30.89% examples, 1012281 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:10,393 : INFO : PROGRESS: at 32.11% examples, 1013382 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:11,395 : INFO : PROGRESS: at 33.42% examples, 1017522 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:12,398 : INFO : PROGRESS: at 34.67% examples, 1019549 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:13,405 : INFO : PROGRESS: at 35.94% examples, 1021852 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:14,406 : INFO : PROGRESS: at 37.02% examples, 1018622 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:15,413 : INFO : PROGRESS: at 38.16% examples, 1017196 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:16,419 : INFO : PROGRESS: at 39.37% examples, 1017430 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:17,421 : INFO : PROGRESS: at 40.57% examples, 1018205 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:18,421 : INFO : PROGRESS: at 41.75% examples, 1017787 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:19,424 : INFO : PROGRESS: at 42.92% examples, 1016924 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:20,435 : INFO : PROGRESS: at 44.10% examples, 1016465 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:21,437 : INFO : PROGRESS: at 45.27% examples, 1016066 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:22,439 : INFO : PROGRESS: at 46.43% examples, 1015313 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:23,443 : INFO : PROGRESS: at 47.65% examples, 1015658 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:24,443 : INFO : PROGRESS: at 48.85% examples, 1016051 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:25,443 : INFO : PROGRESS: at 50.12% examples, 1017612 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:26,446 : INFO : PROGRESS: at 51.38% examples, 1019172 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:27,449 : INFO : PROGRESS: at 52.58% examples, 1019375 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:28,450 : INFO : PROGRESS: at 53.85% examples, 1021015 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:29,454 : INFO : PROGRESS: at 55.13% examples, 1022682 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:30,465 : INFO : PROGRESS: at 56.35% examples, 1022942 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:31,469 : INFO : PROGRESS: at 57.57% examples, 1023306 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:32,475 : INFO : PROGRESS: at 58.79% examples, 1023934 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:33,476 : INFO : PROGRESS: at 60.06% examples, 1025196 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:34,485 : INFO : PROGRESS: at 61.21% examples, 1024190 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:35,490 : INFO : PROGRESS: at 62.33% examples, 1022751 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:36,494 : INFO : PROGRESS: at 63.44% examples, 1021221 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:37,496 : INFO : PROGRESS: at 64.48% examples, 1018884 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:38,498 : INFO : PROGRESS: at 65.80% examples, 1020766 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:39,500 : INFO : PROGRESS: at 67.17% examples, 1023281 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:40,502 : INFO : PROGRESS: at 68.52% examples, 1025408 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:41,504 : INFO : PROGRESS: at 69.79% examples, 1026349 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:42,506 : INFO : PROGRESS: at 71.08% examples, 1027836 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:43,510 : INFO : PROGRESS: at 72.34% examples, 1028668 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:44,514 : INFO : PROGRESS: at 73.66% examples, 1030381 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:45,518 : INFO : PROGRESS: at 74.99% examples, 1032164 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:46,525 : INFO : PROGRESS: at 76.32% examples, 1033849 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:47,527 : INFO : PROGRESS: at 77.58% examples, 1034448 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:48,528 : INFO : PROGRESS: at 78.87% examples, 1035698 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:49,537 : INFO : PROGRESS: at 80.16% examples, 1036680 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:50,538 : INFO : PROGRESS: at 81.49% examples, 1038100 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:51,539 : INFO : PROGRESS: at 82.79% examples, 1039071 words/s, in_qsize 8, out_qsize 0\n",
      "2017-02-08 14:49:52,542 : INFO : PROGRESS: at 84.13% examples, 1040597 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:53,543 : INFO : PROGRESS: at 85.48% examples, 1042094 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:54,550 : INFO : PROGRESS: at 86.82% examples, 1043490 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:55,559 : INFO : PROGRESS: at 88.15% examples, 1044511 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:56,567 : INFO : PROGRESS: at 89.45% examples, 1045507 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:57,574 : INFO : PROGRESS: at 90.71% examples, 1045702 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:58,577 : INFO : PROGRESS: at 91.97% examples, 1046213 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:49:59,584 : INFO : PROGRESS: at 93.30% examples, 1047434 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:50:00,593 : INFO : PROGRESS: at 94.62% examples, 1048488 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:50:01,593 : INFO : PROGRESS: at 95.87% examples, 1048819 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:50:02,599 : INFO : PROGRESS: at 97.17% examples, 1049608 words/s, in_qsize 6, out_qsize 1\n",
      "2017-02-08 14:50:03,605 : INFO : PROGRESS: at 98.48% examples, 1050445 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:50:04,611 : INFO : PROGRESS: at 99.72% examples, 1050561 words/s, in_qsize 7, out_qsize 0\n",
      "2017-02-08 14:50:04,807 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-02-08 14:50:04,814 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-02-08 14:50:04,815 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-02-08 14:50:04,818 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-02-08 14:50:04,819 : INFO : training on 118594935 raw words (85703140 effective words) took 81.6s, 1050838 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \n",
    "                         size=num_features, min_count=min_word_count,\n",
    "                         window=context, sample=downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-08 14:50:04,824 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-02-08 14:50:04,979 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2017-02-08 14:50:04,980 : INFO : not storing attribute cum_table\n",
      "2017-02-08 14:50:04,981 : INFO : not storing attribute syn0norm\n",
      "2017-02-08 14:50:05,563 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paris'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"paris berlin london austria\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6129059791564941),\n",
       " ('lad', 0.5809935927391052),\n",
       " ('lady', 0.5616000294685364),\n",
       " ('monk', 0.4992210268974304),\n",
       " ('farmer', 0.49889662861824036),\n",
       " ('men', 0.49541759490966797),\n",
       " ('guy', 0.4946671724319458),\n",
       " ('millionaire', 0.4924411475658417),\n",
       " ('businessman', 0.4894394278526306),\n",
       " ('person', 0.487354576587677)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('princess', 0.6044710874557495),\n",
       " ('bride', 0.5868712663650513),\n",
       " ('mistress', 0.5574051141738892),\n",
       " ('goddess', 0.5508238673210144),\n",
       " ('duchess', 0.5441114902496338),\n",
       " ('victoria', 0.5438050031661987),\n",
       " ('stepmother', 0.5378952026367188),\n",
       " ('maid', 0.5345040559768677),\n",
       " ('showgirl', 0.5275346040725708),\n",
       " ('nun', 0.5255606174468994)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.7490795850753784),\n",
       " ('abysmal', 0.7023686170578003),\n",
       " ('atrocious', 0.7018066644668579),\n",
       " ('horrible', 0.7017901539802551),\n",
       " ('dreadful', 0.6742359399795532),\n",
       " ('horrendous', 0.6662415862083435),\n",
       " ('appalling', 0.6474866271018982),\n",
       " ('horrid', 0.6434177160263062),\n",
       " ('lousy', 0.5972024202346802),\n",
       " ('bad', 0.5899964570999146)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"awful\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
